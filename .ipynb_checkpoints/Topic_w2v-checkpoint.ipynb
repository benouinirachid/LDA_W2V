{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 와 word2Vec 섞은 Topic-word Vector를 만들어 봅시다 \n",
    "\n",
    "- 정의란 무엇인가? 이는 사람이 가지고 있는 지식에 따라 다른 대답이 나올 수 있다. 이는 정의가 동음이의어의 성격을 가지고 있기 때문이다. \n",
    "    - 기본적으로 정의란 justice의 의미로 가장 많이 사용된다. 하지만 이과생들에게는 definition 의미로 정의라는 단어를 많이 사용한다\n",
    "    - 기존의 word2vec에서는 학습 데이터에 따라 정의가 justice나 defination의 중간 어딘가에 매칭될 것이다\n",
    "    - 이러한 단어가 많아질 수록, 벡터 공간의 일부분을 동음이의어가 차지하는 결과를 보일 것이며, 이러한 문제를 해결하기 위해서는 더 많은 차원이 필요해 질 것이다. \n",
    "\n",
    "- 따라서, 정의라는 단어는 주제적인 측면에서 1차적으로 분류되고, 그 다음 의미적인 분석을 수행해야 한다 \n",
    "    - 그래서 아이디어는 LDA를 활용하여 1차적으로 k개의 잠재적 토픽으로 분류하고, 이를 바탕으로 k X W x V의 Word2vec 학습을 수행한다 \n",
    "    - 즉, one-hot encoding 을 통해 합쳐질 때, k의 가중치를 반영하여 여러 w x v 메트릭스를 동시에 학습하는 것이다. \n",
    "    \n",
    "- 우선 구현은 다른 라이브러리를 적극적으로 활용하느 것으로 하되, 여의치 않으면 작은 사이즈로 CBOW만 구현 해보는 것으로 하자  \n",
    "\n",
    "---\n",
    "\n",
    "- 데이터 전처리 -> LDA(Vocab의 사이즈는 LDA와 W2V이랑 동일) -> k(topic size) X W(vocab size) X vector(word vector size)   \n",
    "\n",
    "---\n",
    "  \n",
    "참조 사이트 \n",
    "- https://hulk89.github.io/neural%20machine%20translation/2017/05/08/Word2Vec-impl/ \n",
    "- http://solarisailab.com/archives/374\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library definition \n",
    "import collections \n",
    "import math \n",
    "import os \n",
    "import zipfile \n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: 텍스트 데이터 읽어오기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data read \n",
    "def read_Amazon_review_data(file_path):\n",
    "    review_list = []\n",
    "    with open (file_path) as json_file:\n",
    "        d = json_file.read()\n",
    "\n",
    "    split_data = d.split(\"\\n\")\n",
    "    for text in split_data:\n",
    "        if text.strip():\n",
    "            text_list = json.loads(text)\n",
    "            review_list.append({\"ID\": text_list[\"reviewerID\"], \n",
    "                                \"Review\": text_list[\"reviewText\"], \n",
    "                                \"Rating\": text_list[\"overall\"], \n",
    "                                \"asin\": text_list[\"asin\"],\n",
    "                                \"timestemp\":text_list[\"unixReviewTime\"]\n",
    "                                         \n",
    "                               })\n",
    "\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = read_Amazon_review_data(\"reviews_Musical_Instruments_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'A2IBPI20UZIR0U',\n",
       "  'Review': \"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\",\n",
       "  'Rating': 5.0,\n",
       "  'asin': '1384719342',\n",
       "  'timestemp': 1393545600},\n",
       " {'ID': 'A14VAT5EAX3D9S',\n",
       "  'Review': \"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\",\n",
       "  'Rating': 5.0,\n",
       "  'asin': '1384719342',\n",
       "  'timestemp': 1363392000}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word = [] \n",
    "for line in read_data: \n",
    "    line[\"Review\"] = list(gensim.utils.simple_preprocess(str(line[\"Review\"]), deacc=True))\n",
    "    data_word.append(line[\"Review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: LDA 수행하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "\n",
    "bigram = gensim.models.Phrases(data_word, min_count = 5, threshold = 100)\n",
    "bigram_mode = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def remove_stopwords(texts): \n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mode[doc] for doc in texts]\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_word)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = gensim.corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "id2word=id2word,\n",
    "num_topics=20,\n",
    "random_state=100,\n",
    "update_every=1,\n",
    "chunksize=100,\n",
    "passes=10,\n",
    "alpha='auto',\n",
    "per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"less\" + 0.015*\"pick\" + 0.014*\"picks\" + 0.013*\"however\" + 0.012*\"give\" + 0.011*\"big\" + 0.010*\"distortion\" + 0.010*\"clip\" + 0.010*\"accurate\" + 0.010*\"power\"'),\n",
       " (1,\n",
       "  '0.032*\"star\" + 0.031*\"tight\" + 0.024*\"wall\" + 0.023*\"bottom\" + 0.020*\"version\" + 0.017*\"violin\" + 0.016*\"speakers\" + 0.016*\"screws\" + 0.016*\"hd\" + 0.013*\"micro\"'),\n",
       " (2,\n",
       "  '0.111*\"pedal\" + 0.091*\"tone\" + 0.042*\"pedals\" + 0.024*\"gain\" + 0.021*\"joyo\" + 0.019*\"overdrive\" + 0.017*\"battery\" + 0.017*\"knob\" + 0.014*\"tones\" + 0.013*\"effect\"'),\n",
       " (3,\n",
       "  '0.043*\"many\" + 0.036*\"volume\" + 0.033*\"effects\" + 0.031*\"noise\" + 0.028*\"tube\" + 0.023*\"amps\" + 0.021*\"output\" + 0.020*\"software\" + 0.019*\"trying\" + 0.018*\"loud\"'),\n",
       " (4,\n",
       "  '0.047*\"bright\" + 0.035*\"especially\" + 0.029*\"compared\" + 0.025*\"setting\" + 0.023*\"smaller\" + 0.023*\"designed\" + 0.021*\"stands\" + 0.019*\"prefer\" + 0.019*\"adjustable\" + 0.017*\"pickup\"'),\n",
       " (5,\n",
       "  '0.051*\"music\" + 0.037*\"studio\" + 0.036*\"amazon\" + 0.033*\"inexpensive\" + 0.027*\"level\" + 0.026*\"feature\" + 0.025*\"large\" + 0.024*\"fast\" + 0.023*\"takes\" + 0.017*\"mixer\"'),\n",
       " (6,\n",
       "  '0.146*\"strings\" + 0.055*\"string\" + 0.047*\"tune\" + 0.037*\"acoustic\" + 0.031*\"new\" + 0.030*\"tuning\" + 0.024*\"tuners\" + 0.023*\"addario\" + 0.014*\"ever\" + 0.010*\"open\"'),\n",
       " (7,\n",
       "  '0.075*\"could\" + 0.061*\"instrument\" + 0.050*\"low\" + 0.045*\"makes\" + 0.034*\"excellent\" + 0.029*\"happy\" + 0.025*\"easier\" + 0.021*\"purchase\" + 0.021*\"cool\" + 0.020*\"range\"'),\n",
       " (8,\n",
       "  '0.097*\"capo\" + 0.037*\"tension\" + 0.037*\"snark\" + 0.027*\"head\" + 0.023*\"ukulele\" + 0.023*\"headstock\" + 0.021*\"fret\" + 0.021*\"deal\" + 0.019*\"capos\" + 0.018*\"sn\"'),\n",
       " (9,\n",
       "  '0.045*\"recording\" + 0.034*\"metal\" + 0.025*\"side\" + 0.025*\"record\" + 0.024*\"usb\" + 0.023*\"microphone\" + 0.022*\"ipad\" + 0.022*\"switch\" + 0.021*\"issue\" + 0.018*\"stay\"'),\n",
       " (10,\n",
       "  '0.100*\"playing\" + 0.045*\"problem\" + 0.030*\"mine\" + 0.028*\"point\" + 0.026*\"tell\" + 0.019*\"fairly\" + 0.017*\"weight\" + 0.015*\"seen\" + 0.015*\"musician\" + 0.013*\"certainly\"'),\n",
       " (11,\n",
       "  '0.067*\"job\" + 0.066*\"hold\" + 0.037*\"computer\" + 0.026*\"mini\" + 0.022*\"received\" + 0.022*\"construction\" + 0.021*\"padding\" + 0.020*\"holder\" + 0.018*\"adapter\" + 0.017*\"xlr\"'),\n",
       " (12,\n",
       "  '0.086*\"see\" + 0.061*\"strap\" + 0.034*\"button\" + 0.030*\"bag\" + 0.022*\"comfortable\" + 0.020*\"tracks\" + 0.018*\"together\" + 0.015*\"buttons\" + 0.014*\"spot\" + 0.014*\"manual\"'),\n",
       " (13,\n",
       "  '0.057*\"guitar\" + 0.043*\"great\" + 0.039*\"good\" + 0.033*\"sound\" + 0.024*\"little\" + 0.022*\"price\" + 0.021*\"really\" + 0.020*\"also\" + 0.020*\"better\" + 0.019*\"quality\"'),\n",
       " (14,\n",
       "  '0.122*\"amp\" + 0.063*\"sounds\" + 0.058*\"think\" + 0.037*\"say\" + 0.033*\"clean\" + 0.032*\"box\" + 0.026*\"old\" + 0.024*\"sound\" + 0.023*\"sounding\" + 0.017*\"rock\"'),\n",
       " (15,\n",
       "  '0.067*\"one\" + 0.040*\"get\" + 0.037*\"like\" + 0.034*\"well\" + 0.032*\"would\" + 0.027*\"time\" + 0.024*\"used\" + 0.022*\"play\" + 0.022*\"even\" + 0.018*\"still\"'),\n",
       " (16,\n",
       "  '0.042*\"right\" + 0.041*\"back\" + 0.040*\"first\" + 0.036*\"high\" + 0.030*\"end\" + 0.023*\"cable\" + 0.022*\"worth\" + 0.022*\"money\" + 0.020*\"board\" + 0.019*\"comes\"'),\n",
       " (17,\n",
       "  '0.038*\"plastic\" + 0.034*\"tascam\" + 0.032*\"times\" + 0.031*\"type\" + 0.030*\"already\" + 0.027*\"pitch\" + 0.025*\"jam\" + 0.023*\"speaker\" + 0.021*\"th\" + 0.020*\"noticed\"'),\n",
       " (18,\n",
       "  '0.092*\"stand\" + 0.071*\"mic\" + 0.044*\"device\" + 0.038*\"display\" + 0.036*\"came\" + 0.020*\"wish\" + 0.019*\"must\" + 0.017*\"highly_recommend\" + 0.017*\"inside\" + 0.017*\"stage\"'),\n",
       " (19,\n",
       "  '0.095*\"tuner\" + 0.053*\"bass\" + 0.048*\"best\" + 0.047*\"make\" + 0.030*\"keep\" + 0.029*\"found\" + 0.025*\"top\" + 0.023*\"unit\" + 0.020*\"either\" + 0.017*\"place\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: word2vec 구현하기 using tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_input(dataset,id2word, window_size):\n",
    "    random.shuffle(dataset)\n",
    "    data = []\n",
    "    label = []\n",
    "    for doc in dataset : \n",
    "        for idx in range(int(window_size/2), len(doc)-(window_size - int(window_size/2))):\n",
    "            front = idx - int(window_size/2)\n",
    "            rear = idx + (window_size - int(window_size/2))\n",
    "            #flatten.append({'data': doc[front:idx] + doc[idx:rear], 'label':doc[idx]})\n",
    "            data.append(id2word.doc2idx(doc[front:idx] + doc[idx+1:rear])) \n",
    "            label.append(id2word.doc2idx([doc[idx]]))\n",
    "    \n",
    "    return(data, label)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(id2word)\n",
    "embedding_size = 5\n",
    "input_ , label = generate_input(texts, id2word, embedding_size)\n",
    "batch_size = len(label)\n",
    "num_sampled = vocab_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96, 120, 1675, 121], [120, 183, 121, 248]] \n",
      "\n",
      " [[183], [1675]]\n"
     ]
    }
   ],
   "source": [
    "print(input_[:2], '\\n\\n',label[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size-1], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size-1], stddev = 1.0/ math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size, embedding_size-1])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for 'nce_loss_1/MatMul' (op: 'MatMul') with input shapes: [393318,4,4], [9593,4].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 3 for 'nce_loss_1/MatMul' (op: 'MatMul') with input shapes: [393318,4,4], [9593,4].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-62945ab57615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      \u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                      \u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                      vocab_size))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mnce_loss\u001b[0;34m(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0mremove_accidental_hits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_accidental_hits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m       \u001b[0mpartition_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1247\u001b[0m   sampled_losses = sigmoid_cross_entropy_with_logits(\n\u001b[1;32m   1248\u001b[0m       labels=labels, logits=logits, name=\"sampled_losses\")\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36m_compute_sampled_logits\u001b[0;34m(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;31m# sampled_w has shape [num_sampled, dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;31m# Apply X*W', which yields [batch_size, num_sampled]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m     \u001b[0msampled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;31m# Retrieve the true and sampled biases, compute the true logits, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2018\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4454\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4455\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4456\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4457\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4458\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1731\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 3 for 'nce_loss_1/MatMul' (op: 'MatMul') with input shapes: [393318,4,4], [9593,4]."
     ]
    }
   ],
   "source": [
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights,\n",
    "                                     nce_biases,\n",
    "                                     train_labels,\n",
    "                                     embed,\n",
    "                                     num_sampled,\n",
    "                                     vocab_size))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(1000):\n",
    "        batch_inputs, batch_labels = generate_input(texts, id2word,5)\n",
    "        print(batch_inputs[:5],'\\n', batch_labels[:5])\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    \n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if i % 100 == 0:\n",
    "            print(loss_val)\n",
    "    emb_weights = sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['나는',   '그녀가',  '너는', '그가',\n",
    "         '밥을', '콩밥을', '싸움을', '꽃을', \n",
    "         '먹었다', '했다',  '샀다', '만들었다']\n",
    "dataset = [[0, 4, 8], [1, 4, 8], [2, 4, 8], [3, 4, 8],\n",
    "           [0, 5, 8], [0, 5, 9], [1, 5, 8], [1, 5, 9],\n",
    "           [2, 5, 8], [2, 5, 9], [3, 5, 8], [3, 5, 9],\n",
    "           [0, 6, 9], [1, 6, 9], [2, 6, 9], [3, 6, 9],\n",
    "           [0, 7, 10], [1, 7, 10], [2, 7, 10], [3, 7, 10], \n",
    "           [0, 6, 11], [1, 6, 11], [2, 6, 11], [3, 6, 11]]\n",
    "\n",
    "def decode_data(data, vocab):  \n",
    "    '''\n",
    "    idx들의 list로 문장을 만들어주는 함수\n",
    "    '''\n",
    "    decoded_list = [vocab[idx] for idx in data]\n",
    "    return ' '.join(decoded_list)\n",
    "\n",
    "for data in dataset[0:5]:   # 데이터를 몇개만 찍어보자.\n",
    "    print(decode_data(data, vocab))\n",
    "    \n",
    "def generate_input(dataset, num_skips):\n",
    "    random.shuffle(dataset)  # 문장 단위로 셔플한다.\n",
    "\n",
    "    # 일차원 array로 만든다. (window를 돌리기 위해!)\n",
    "    flatten = []\n",
    "    for list_ in dataset:\n",
    "        flatten += list_\n",
    "\n",
    "    # (나는, 그녀를 보았다.) => (i:그녀를, l:나는), (i:그녀를, l:보았다)\n",
    "    data = []\n",
    "    label = []\n",
    "    for idx in range(num_skips, len(flatten)-num_skips):\n",
    "        data.append(flatten[idx])\n",
    "        data.append(flatten[idx])\n",
    "        label.append([flatten[idx-1]])\n",
    "        label.append([flatten[idx+1]])\n",
    "    return data, label\n",
    "\n",
    "input_, label = generate_input(dataset, 1)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 5\n",
    "batch_size = len(label)\n",
    "num_sampled = 6  # vocab_size//2\n",
    "\n",
    "## Graph build\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size,\n",
    "                                            embedding_size],\n",
    "                                           -1.0, 1.0))\n",
    "\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocab_size,\n",
    "                                               embedding_size],\n",
    "                                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "# Placeholders for inputs\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# 매번 음수 라벨링 된 셈플을 이용한 NCE loos 계산\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights,\n",
    "                                     nce_biases,\n",
    "                                     train_labels,\n",
    "                                     embed,\n",
    "                                     num_sampled,\n",
    "                                     vocab_size))\n",
    "\n",
    "# SGD optimizer 를 사용\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(1000):\n",
    "        batch_inputs, batch_labels = generate_input(dataset, 1)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    \n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if i % 100 == 0:\n",
    "            print(loss_val)\n",
    "    emb_weights = sess.run(embeddings)\n",
    "\n",
    "np.savetxt('dataset', emb_weights, fmt='%.5e', delimiter='\\t')\n",
    "with open('meta', 'w') as f:\n",
    "    for i in vocab:\n",
    "        f.write(\"{}\\n\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: LDA 수행 결과를 반영하여 word2Vec 수행하기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_test",
   "language": "python",
   "name": "tensor_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
